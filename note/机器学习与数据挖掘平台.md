# 机器学习与数据挖掘平台

# 数据准备

数据清洗（Data Cleansing）+特征工程（Feature Engineering）

数据准备是数据科学的核心。它包括数据清洗和特征工程。另外领域知识（domain knowledge）也非常重要，它有助于获得好的结果。数据准备不能完全自动化，至少在初始阶段不能。通常，数据准备占去整个分析管道（流程）的60％到80％。但是，为了使机器学习算法在数据集上获得最优的精确性，数据准备必不可少。

数据清洗可使数据获得用于分析的正确形状（shape）和质量（quality）。它包括了许多不同的功能，例如：

+ 基本功能（选择、过滤、去重、...）
+ 采样（平衡（balanced）、分层（stratified）、...）
+ 数据分配（创建训练+验证+测试数据集、...）
+ 变换（归一化、标准化、缩放、pivoting、...）
+ 分箱（Binning）（基于计数、将缺失值作为其自己的组处理、...）
+ 数据替换（剪切（cutting）、分割（splitting）、合并、...））
+ 加权与选择（属性加权、自动优化、...）
+ 属性生成（ID生成、...）
+ 数据填补（imputation）（使用统计算法替换缺失的观察值）

特征工程会为分析选取正确的属性。我们需要借助数据的领域知识来选取或创建属性，这些属性能使机器学习算法正确地工作。特征工程过程包括：

+ 头脑风暴或特征测试
+ 特征选择
+ 验证这些特征如何与模型配合使用
+ 如果需要，改进特征
+ 回到头脑风暴/创建更多的特征，直到工作完成

请注意，特征工程已是建模（构建分析模型）步骤里的一部分，但它也利用数据准备这一功能（例如提取字符串的某些部分）。

数据清洗和特征工程是数据准备的一部分，也是机器学习和深度学习应用的基础。这二者并不是那么容易，都需要花费功夫。

数据准备会出现在分析项目的不同阶段：

+ 数据预处理：从数据源获取数据之后直接处理数据。通常由开发人员或数据科学家实现，它包括初始转换、聚合（aggregation）和数据清洗。此步骤在数据的交互式分析开始之前完成。它只执行一次。
+ 数据整理：在交互式数据分析和建模期间准备数据。通常由数据科学家或业务分析师完成，以便更改数据集和特征工程的视图。此步骤会迭代更改数据集的形状，直到它能很好地查找洞察或构建良好的分析模型。

目前市场上有各种各样的框架和工具。它们都以这种或那种方式支持类似Hadoop或Spark的大数据框架。举几个例子：

+ 数据获取开源框架（仅关注数据获取和预处理步骤）：Apache NiFi、StreamSets、Cask Hydrator
+ 流式处理开源框架（完整的流式分析流程）：Apache Storm、Apache Flink、Apache Apex
+ 流式处理商业软件（完整的流式分析流程）：Software AG Apama、IBM Streams、TIBCO StreamBase

使用这些工具（包括ETL）的巨大优势是，你可以使用同一套工具或框架（对历史数据）进行数据预处理，以及（对新数据）进行实时处理（以便在变化的数据里使用分析模型）。这将会是一个不错的选择，用户不仅可以保持小而精的工具集，而且还能通过一套工具同时获得ETL/获取和实时处理。